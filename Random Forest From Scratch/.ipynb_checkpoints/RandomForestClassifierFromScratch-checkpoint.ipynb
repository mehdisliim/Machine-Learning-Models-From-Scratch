{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    pipe_enp = Pipeline([('imp', SimpleImputer(strategy='most_frequent')),\n",
    "                        ('enc', OrdinalEncoder())])\n",
    "    \n",
    "    df['nFam'] = df['SibSp'].copy() + df['Parch'].copy()\n",
    "    df['Tag'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "    df['Cabin'] = df.Cabin.apply(lambda x: x[0] if pd.notna(x) else 'NAN')\n",
    "    df['Sex'] = df['Sex'].apply(lambda x: 1 if x=='female' else 0)\n",
    "    df.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket'], inplace=True, axis=1),\n",
    "                     \n",
    "                     \n",
    "\n",
    "    pipe_imp_age = SimpleImputer()\n",
    "\n",
    "    df[['Embarked', 'Tag', 'Cabin']] = pipe_enp.fit_transform(np.c_[df[['Embarked', 'Tag', 'Cabin']]])\n",
    "\n",
    "\n",
    "    df[['Age', 'Fare']] = pipe_imp_age.fit_transform(np.c_[df[['Age', 'Fare']]])\n",
    "    Cat_cols = ['Sex', 'Pclass', 'Cabin', 'Embarked', 'Tag' ]\n",
    "    CounN_cols = ['Age', 'Fare', 'nFam']\n",
    "    df.Fare = df.Fare.apply(lambda x: df.Fare.mean() if x == 0 else x)\n",
    "    features, labels = df.iloc[:, 1:], np.c_[df.iloc[:, 0]]\n",
    "    features = np.c_[(features - features.mean()) / features.std()]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import openml\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "titanic = pd.read_csv('train.csv') #Load the data\n",
    "X, y = preprocess(titanic)#it imputes, encodes and scales our dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) #split the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3><b>Random Forest Classifier Big Picture & Technical stuff:</b></h3></center>\n",
    "\n",
    "<b>Introduction : </b>\n",
    "\tFirst of all we gonna introduce the Big Picture of Decision Trees which is the learning technique that Random Forest built upon and we gonna dive in on how the Decision Tree really work after that we gonna show how Random Forest works.  Don't worry there is not a lot of math evolved there is just little bit of multplication and summation :)) so enjoy reading!<br><br>\n",
    "<b>Big Picture : </b>\n",
    "        Say you wanna make a function (algorithm) that takes an information about a fruit (color, weight, shape) and outputs whether the given sample (color, weight, shape) is a banana or a red apple the algorithm needs to find that the best way to differ between a banana and a red apple is the color (the shape too but let's pretend that the color is the best feature to separate between these two labels ) so the algorithm should learn that if a given sample has color equals to yellow then it's a banana else it's a red apple, say now we have 3 kind of labels red apples, oranges and bananas doing the same approach it will find that if the color of the sample is yellow then it's banana now we took care about how to label a sample to the banana class then it moves to how to separate the remaining classes and it will find that if color of the sample is red then it's apple else orange(or if the color of the sample is orange then it's an orange else it's an apple), okay that was simple and we could just type that using an if statements but imagine if you had 1000 type of fruits, probably you'll get bored typing all of these if statments right? and that would be hard too! that's were decision tree comes in.\n",
    "<img src=\"dct.jpg\" width=\"750\" height=\"500\" align=\"center\"/>\n",
    "<h5><b>The Below Picture shows some important Terminology </b></h5>\n",
    "<img src=\"Terminology.png\" width=\"750\" height=\"500\" align=\"center\"/><br><br>\n",
    "Decision Trees works as follows it finds the best feature (threshold's column) and it's specific value (threshold) that best separate the classes (that's the root node which is the first decision node that separates the dataset) now we end up having dataset that have value below a certain specific value (threshold) in that feature (threshold's column) and dataset that have value above a certain specific value (threshold) in that feature (threshold's column) now for each of the two datasets check if they are pure meaning they have only one class in them say the dataset that was above threshold was pure and it had only the class A it then we save that and we say for each new sample if it had in the threshold's column value above the threshold then it's belongs to the class A (that's the Leaf node), back to our two new datasets (dataset below/above threshold) if one of these datasets or both wasn't pure we repeat the process on the dataset/s that wasn't pure, for example data below threshold wasn't pure then we take it we find the best threshold that separate the classes in it then get our datasets that above and below threshold checks wether they were pure if indeed they were pure then we produce 2 leaf nodes(one for the class that the dataset above the last threshold have and the other for the class that the dataset below the last threshold have), if the two or one of them wasn't pure we repeat until we get pure datasets\n",
    "\n",
    "<b>Being Technical</b> : In this part we will illustrate what every method (function) of the class DecisionTreeClassifier does then we will show how they all work together in a smooth way to create our Potential random Forest:\n",
    "\n",
    "<h5><b> DecisionTreeClassifier's methods illustrated : <b></h5><br>\n",
    "{'function_name' : ispure, 'what_it_does': 'it checks wether a given array contains a single class (True) or if it has more than 1 class (False)', 'examples': ispure(np.array([1, 1, 1, 1])) returns True ,ispure(np.array([1, 2, 5, 2])) returns False}<br><br>\n",
    "{'function_name': most_frequent, 'what_it_does':'it returns the most frequent value in the given array', 'example':most_frequent(np.array([25, 6, 2, 3, 2, 5, 4, 2])) returns 2}<br><br>\n",
    "{'function_name': impurity_measure, 'what_it_does': 'basically you can think of it as a measure of how different the values are in the array where using entropy equation calculates how random the values in the array are and gini calculates how random the values in the array would be if they were sampled from random distribution', 'example': impurity_measure(np.array([2, 2, 2, 2, 2])) whether we used entropy or gini equation it will return 0 since the values in the array are not different and if it returned value > 0 in another example that means the values of the given array are different}<br><br>\n",
    "{'function_name': overall_impurity, 'what_it_does': it calculates how well a given threshold separates the data by giving it the labels of the samples that have a value above the threshold in the threshold's column and the labels of the samples that have a value below that threshold in the threshold's column, 'example': 'overall_impurity(np.array([0, 0, 0, 0, 0]), np.array([1, 1, 1, 1, 1])) it returns 0 because that threshold pefectly separates the two labels if returned value > 0 in another case that means that the threshold doesn't separate the labels perfectly there is impurity'}<br><br>\n",
    "{'function_name': all_thresholds, 'what_it_does' :'giving it a numpy 2D array for each column it does (ARRAY[row, column]+ARRAY[row-1, column])/2 with for each row > 0 that means it calculates the potential thresholds that could separate the dataset's classes, 'example':'say this is your values in the first column [1, 2, 3, 4] it makes a key in the dictinary has the index of that column and its potential thresholds for example for the fisrt column the dictinary would be {0: [(1+2)/2, (2+3)/2, (3+4)/2]} which equals to {0:[1.5, 2.5, 3.5]}'}<br><br>\n",
    "{'function_name': calculate_best_threshold, 'what_it_does':'from the potential thresholds it calculates the best threshold  that separates the classes in the current dataset'}<br><br>\n",
    "{'function_name': create_tree, 'what_it_does': 'it creates our beloved Decision Tree, it's a recursive function and the best way to understand it is to go through it, max_depth argument stops the the decision from growing when it reaches that depth, min_samples_split is removing potential sub-tree and replace it with the most frequent class in that dataset when the length of the dataset reachs that min_samples_split and these argument are just regularizations to avoid overfitting'}<br><br>\n",
    "{'function_name': split_data_using_threshold, 'what_it_does': 'returns the data below the threshold in the threshold's column and the data above the threshold in the threshold's column'},\n",
    "{'function_name': predict_sample, 'what_it_does': 'returns the prediction of the giving sample, it's recursive function too'},\n",
    "{'function_name': predict, 'what_it_does': 'returns the prediction of the giving samples/sample'},\n",
    "{'function_name': col_threshold, 'what_it_does': 'it helps us to get from the way the tree is stored the threshold and the column of the node that we are in to move to the next node (we use in the prediction part)'}\n",
    "    \n",
    "\n",
    "<br><br>\n",
    "so how they all work together? alright focus with me now, we start by calculating the best threshold and it's column that best separates the data then we get the data that has value above the threshold in the threshold's column and the data that has value below the threshold in the threshold's column then we check wether the data_below_the_threshold or data_above_the_threshold is pure or satisfies one of the regularizations if yes we put as leaf the most frequent class in that dataset else the data_below/above is not pure so it goes in the create_tree and this whole process repeats until the two datasets make in a recursive iteration one/two of the regularizations satisfied or both of the datasets are pure, if you didn't understand that it's REALLY okay, become familiar with recursivity and then go through the create_tree using pen and paper and hopefully you'll understand it.<br><br>\n",
    "<b>Notes : </b>\n",
    "    - use the example of both data_below_threshold and data_above_threshold being pure from the first recursive iteration, doing that will help you understand the creation of the Decision Tree\n",
    "    - Decision trees has low bias and high variance that means they don't care if the data was quadratic or linear or ... and high variance basically it means they overfit(most of the time), Linear Regression model has high bias and low variance since Linear regression expects the data to be linear and it will have low variance if the data was indeed linear\n",
    "<h5><b> RandomForest's Big Picture :<b></h5><br>\n",
    "    Random Forest is just a bunch of decision trees each tree has random samples associated with random columns from the dataset for example say our dataset has 1000 sample and 5 columns then our first decision tree may have 780  random samples and the columns 1 , 2 and 4, the second decision tree will have 780 random sample and the columns may be 1, 3 and 5 and so on ..., the number of Decision Trees in the Random Forest is controlled by the argument n_estimators, how many samples you want to train the Decision Tree on is controlled by the argument max_samples_ratio you give it value between (0..1] and it calculates how many samples it must give to the decision tree for example max_samples_ratio = 0.7 that means give the decision tree 70% of all the samples and how many column it must feed the tree is controlled by the max_features_ratio, for example if max_samples_ratio = 0.6 and max_features_ratio = 0.8 that means feed each Decision Tree random 60% of all the samples and random 80% of all the features (columns), and the way it classifies the class it runs the sample in each Decision Tree and saves the output then after going through each tree it outputs the most frequent predicted class for example say we have Random Forest with 10 trees we give each one of them the same sample, 7 trees predicts that this sample belongs to 1's class and the other 3 trees says that this sample belongs to the 2's class so the Random Forest outputs 1 since most of the trees predicted 1 for the sample.\n",
    "<h5><b> But why is this approach is better than a simple Decision Tree?<b></h5>\n",
    "By feeding random samples and columns from the dataset to the trees they will generalize in different ways and that's great that means each tree has it strength and it's weakness so if one made a mistake the other trees will help to make the correct classification, Nevertheless they make commun errors, by doing that it will solve a little bit of the high variance problem in the Decision Tree.\n",
    "<center><h5><b>Random Forest's Error Space Expectation versus Reality</b></h5></center>\n",
    "<img src=\"EvsR.jpg\" width=\"750\" height=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(object):\n",
    "    def __init__(self, max_depth=6, min_samples_split=3, imp_measure='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.imp_measure = imp_measure\n",
    "        self.tree = None\n",
    "    \n",
    "    def impurity_measure(self, y, imp_measure='gini'):\n",
    "        nichts, data_points = np.unique(y, return_counts=True) \n",
    "        p = data_points / sum(data_points)\n",
    "        if imp_measure == 'entropy':\n",
    "            return sum(-p*np.log2(p))\n",
    "        elif imp_measure == 'gini':\n",
    "            return 1-sum(np.square(p))\n",
    "    \n",
    "    def calculate_best_threshold(self, X, y, impurity_measure='gini'):\n",
    "        X = np.c_[X]\n",
    "        all_threshs = self.all_thresholds(X)\n",
    "        maxi = np.inf\n",
    "        best_threshold = 0\n",
    "        its_column = 0\n",
    "        for key in all_threshs.keys():\n",
    "            for threshold in all_threshs[key]:\n",
    "                y_above, y_below = y[X[:, key] >= threshold ], y[X[:, key] < threshold ]\n",
    "                overall_imp = self.overall_impurity(y_above, y_below, impurity_measure)\n",
    "                if overall_imp <= maxi:\n",
    "                    maxi = overall_imp\n",
    "                    best_threshold = threshold\n",
    "                    its_column = key\n",
    "        return best_threshold, its_column\n",
    "    \n",
    "    def overall_impurity(self, y1, y2, imp_measure='gini'):\n",
    "        len_y1,  len_y2 = len(y1), len(y2)\n",
    "        len_data = len_y1+len_y2\n",
    "        if imp_measure == 'entropy':\n",
    "            return (len_y1/len_data)*self.impurity_measure(y1, 'entropy')+(len_y2/len_data)*self.impurity_measure(y2, 'entropy')\n",
    "        elif imp_measure == 'gini':\n",
    "            return (len_y1/len_data)*self.impurity_measure(y1, 'gini')+(len_y2/len_data)*self.impurity_measure(y2, 'gini')\n",
    "        else:\n",
    "            raise NameError('{} is not supported as impurity measure please use gini or entropy'.format(imp_measure))\n",
    "    def most_frequent(self, y):\n",
    "        unique_vals, counts_unique_vals = np.unique(y, return_counts=True) #in this case since we told it to return the counts meaning how many each unique value appeared in the array so it will return the unique values and their counts\n",
    "        return unique_vals[np.argmax(counts_unique_vals)]\n",
    "    \n",
    "    def all_thresholds(self, X):\n",
    "        D = {}\n",
    "        for c in range(X.shape[1]):\n",
    "            D[c] = []\n",
    "            for l in range(X.shape[0]):\n",
    "                if l > 0:\n",
    "                    D[c].append((X[l-1, c] + X[l, c])/2)\n",
    "        return D\n",
    "        \n",
    "    def ispure(self, y):\n",
    "        unique_vals = np.unique(y)\n",
    "        return len(unique_vals) == 1 \n",
    "\n",
    "    def create_tree(self, X, y, counts=1, max_depth=-1, min_samples_split=-1):\n",
    "        max_depth, min_samples_split=self.max_depth , self.min_samples_split\n",
    "        best_threshold, its_column = self.calculate_best_threshold(X, y, impurity_measure=self.imp_measure)\n",
    "        X_below, y_below, X_above, y_above = self.split_data_using_threshold(X, y, best_threshold, its_column)\n",
    "        if len(y) >= min_samples_split and counts <= max_depth and len(y_above) != 0 and len(y_below) != 0:\n",
    "            counts +=1\n",
    "            if self.ispure(y_below) == False:\n",
    "                yes = self.create_tree(X_below, y_below, counts=counts, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "            else:\n",
    "                yes = self.most_frequent(y_below)\n",
    "            if self.ispure(y_above) == False:\n",
    "                no = self.create_tree(X_above, y_above, counts=counts, max_depth=max_depth, min_samples_split=min_samples_split )\n",
    "            else:\n",
    "                no = self.most_frequent(y_above)\n",
    "            return {str(its_column)+'_'+str(best_threshold) : {True:yes, False:no}}\n",
    "        else:\n",
    "            return {str(its_column)+'_'+str(best_threshold) : {True:self.most_frequent(y), False:self.most_frequent(y)}}\n",
    "   \n",
    "    def predict_sample(self, D, X):\n",
    "        if 'int' in str(type(D)) or 'float' in str(type(D)):\n",
    "            return D\n",
    "        else:\n",
    "            col, thresh = self.col_threshold(tuple(D.keys())[0])\n",
    "            D = D[tuple(D.keys())[0]]\n",
    "            D = D[X[col] < thresh]\n",
    "            return self.predict_sample(D, X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.c_[X] #making sure even if it's one sample it gets in the form of 2D\n",
    "        y_pred = []\n",
    "        for i in range(X.shape[0]):\n",
    "            y_pred.append(self.predict_sample(self.tree, X[i]))\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def train(self, X, y): #make sure X is a 2D array even if they were DataFrames or Series they r converted to 2D so the code workds\n",
    "        X, y = np.c_[X], np.c_[y]\n",
    "        self.tree = self.create_tree(X, y)\n",
    "    \n",
    "    col_threshold = lambda self, a: (int(a[:a.index('_')]), float(a[a.index('_')+1:]))\n",
    "    split_data_using_threshold = lambda self, X, y, threshold, col : (X[X[:, col] < threshold], y[X[:, col] < threshold], X[X[:, col] >= threshold], y[X[:, col] >= threshold] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8100558659217877\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy :',(tree.predict(X_test) == y_test.reshape(-1)).sum()/ X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier(DecisionTreeClassifier):\n",
    "    def __init__(self, max_depth=6, min_samples_split=3,  n_estimators=40, max_features_ratio=1, max_samples_ratio=1, imp_measure='gini', verbose=True):\n",
    "        DecisionTreeClassifier.__init__(self, max_depth=max_depth, min_samples_split=min_samples_split, imp_measure=imp_measure)\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features_ratio = max_features_ratio\n",
    "        self.max_samples_ratio = max_samples_ratio\n",
    "        self.verbose = verbose\n",
    "        self.estimators = None\n",
    "        self.selected_feature_per_estimator = None\n",
    "    \n",
    "    def select_data(self, X, y, n_samples, n_features):\n",
    "        columns = []\n",
    "        while len(columns) < n_features:\n",
    "            r = np.random.randint(0, X.shape[1], 1)\n",
    "            if r not in columns:\n",
    "                columns.append(r)\n",
    "        rows = np.random.randint(0, n_samples, n_samples)\n",
    "        columns = np.array(columns).reshape(-1)\n",
    "        X = X[:, columns]\n",
    "        X = X[rows, :]\n",
    "        y = y[rows]\n",
    "        return X, y, columns\n",
    "            \n",
    "    def train(self, X, y):\n",
    "        \n",
    "        n_features = round(X.shape[1]* self.max_features_ratio)\n",
    "        n_samples = round(X.shape[0]* self.max_samples_ratio)\n",
    "        if self.max_features_ratio < 0 or self.max_features_ratio == 0 or self.max_features_ratio > 1:\n",
    "            raise ValueError('max_features_ratio must be positif between (0..1]')\n",
    "        elif self.max_samples_ratio < 0 or self.max_samples_ratio == 0 or self.max_samples_ratio > 1:  \n",
    "            raise ValueError('max_samples_ratio must be positif between (0..1]')\n",
    "        elif n_features == 0:\n",
    "            raise ValueError('Number of selected features per tree == 0 try to augment the value of max_features_ratio')\n",
    "        elif n_samples == 0:\n",
    "            raise ValueError('Number of selected samples per tree == 0 try to augment the value of max_samples_ratio')\n",
    "        \n",
    "        estimators = []\n",
    "        selected_features_per_estimator = []\n",
    "        for i in range(self.n_estimators):\n",
    "            data_x, data_y, selected_features = self.select_data(X, y, n_samples=n_samples, n_features=n_features)\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split, imp_measure=self.imp_measure )\n",
    "            tree.train(data_x, data_y)\n",
    "            estimators.append(tree)\n",
    "            selected_features_per_estimator.append(selected_features)\n",
    "            if self.verbose:\n",
    "                print('Tree', i+1, 'Done Training!')\n",
    "        self.estimators, self.selected_features_per_estimator = estimators, selected_features_per_estimator\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.estimators == None:\n",
    "            raise Exception('You must train the model before making any prediction! use model.train(X, y)')\n",
    "        else:\n",
    "            y_pred = np.zeros((X.shape[0], len(self.estimators)))\n",
    "            for i in range(len(self.estimators)):\n",
    "                y_pred[:, i] = self.estimators[i].predict(X[:, self.selected_features_per_estimator[i]])\n",
    "            preds = []            \n",
    "            for row in y_pred:\n",
    "                uniques, m_freq = np.unique(row, return_counts=True)\n",
    "                preds.append(uniques[np.argmax(m_freq)])\n",
    "            return preds\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=20, max_features_ratio=0.8, max_samples_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 Done Training!\n",
      "Tree 2 Done Training!\n",
      "Tree 3 Done Training!\n",
      "Tree 4 Done Training!\n",
      "Tree 5 Done Training!\n",
      "Tree 6 Done Training!\n",
      "Tree 7 Done Training!\n",
      "Tree 8 Done Training!\n",
      "Tree 9 Done Training!\n",
      "Tree 10 Done Training!\n",
      "Tree 11 Done Training!\n",
      "Tree 12 Done Training!\n",
      "Tree 13 Done Training!\n",
      "Tree 14 Done Training!\n",
      "Tree 15 Done Training!\n",
      "Tree 16 Done Training!\n",
      "Tree 17 Done Training!\n",
      "Tree 18 Done Training!\n",
      "Tree 19 Done Training!\n",
      "Tree 20 Done Training!\n"
     ]
    }
   ],
   "source": [
    "rfc.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8324022346368715\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy :',(rfc.predict(X_test) == y_test.reshape(-1)).sum()/ X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Our beloved Random Forest did better that a Decision Tree by 2% Great!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
